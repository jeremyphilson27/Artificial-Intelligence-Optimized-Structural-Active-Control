{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PID Parameter Optimization using Q-Learning\n",
    "## Optimizing Kp, Ki, Kd for TF.slx using Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import warnings\n",
    "import h5py\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "import h5py\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: MATLAB Engine Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matlab.engine\n",
    "import os\n",
    "\n",
    "# Start MATLAB engine\n",
    "print(\"Starting MATLAB engine...\")\n",
    "eng = matlab.engine.start_matlab()\n",
    "print(\"MATLAB engine started successfully!\")\n",
    "\n",
    "# Set working directory\n",
    "work_dir = '/home/ma012/code/ai-fp'\n",
    "eng.cd(work_dir, nargout=0)\n",
    "print(f\"Working directory set to: {work_dir}\")\n",
    "\n",
    "# Load EQ1.mat\n",
    "print(\"Loading EQ1.mat...\")\n",
    "eng.load('EQ1.mat', nargout=0)\n",
    "print(\"EQ1.mat loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def run_simulink(Kp, Ki, Kd, eng, print_vars=False):\n",
    "    \"\"\"\n",
    "    Run Simulink model with given PID parameters.\n",
    "\n",
    "    Args:\n",
    "        Kp (float): Proportional gain\n",
    "        Ki (float): Integral gain\n",
    "        Kd (float): Derivative gain\n",
    "        eng: MATLAB engine instance\n",
    "        print_vars (bool): Whether to print MATLAB workspace variables after simulation\n",
    "\n",
    "    Returns:\n",
    "        float: RMSE value from simulation, or None if simulation failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set PID parameters in MATLAB workspace\n",
    "        eng.assignin('base', 'Kp', float(Kp), nargout=0)\n",
    "        eng.assignin('base', 'Ki', float(Ki), nargout=0)\n",
    "        eng.assignin('base', 'Kd', float(Kd), nargout=0)\n",
    "\n",
    "        # Run simulation\n",
    "        out = eng.sim('TF', nargout=1)\n",
    "\n",
    "        # Read RMSE value from out.mat file\n",
    "        # The file contains timeseries data: column 0 = time, column 1 = RMSE values\n",
    "        with h5py.File('out.mat', 'r') as f:\n",
    "            rmse_data = f['ans'][:]  # Extract array with shape (n_samples, 2)\n",
    "            rmse = float(rmse_data[-1, 1])  # Get last RMSE value from column 1\n",
    "\n",
    "        if print_vars:\n",
    "            print_matlab_workspace(eng, title=\"MATLAB Workspace After Simulation\")\n",
    "\n",
    "        return rmse\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Simulation error with Kp={Kp}, Ki={Ki}, Kd={Kd}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"Simulink runner function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulink(Kp, Ki, Kd, eng, print_vars=False):\n",
    "    \"\"\"\n",
    "    Run Simulink model with given PID parameters.\n",
    "\n",
    "    Args:\n",
    "        Kp (float): Proportional gain\n",
    "        Ki (float): Integral gain\n",
    "        Kd (float): Derivative gain\n",
    "        eng: MATLAB engine instance\n",
    "        print_vars (bool): Whether to print MATLAB workspace variables after simulation\n",
    "\n",
    "    Returns:\n",
    "        float: RMSE value from simulation, or None if simulation failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set PID parameters in MATLAB workspace\n",
    "        eng.assignin('base', 'Kp', float(Kp), nargout=0)\n",
    "        eng.assignin('base', 'Ki', float(Ki), nargout=0)\n",
    "        eng.assignin('base', 'Kd', float(Kd), nargout=0)\n",
    "\n",
    "        # Run simulation\n",
    "        out = eng.sim('TF', nargout=1)\n",
    "\n",
    "        # Read RMSE value from out.mat file\n",
    "        # The file contains timeseries data: column 0 = time, column 1 = RMSE values\n",
    "        with h5py.File('out.mat', 'r') as f:\n",
    "            rmse_data = f['ans'][:]  # Extract array with shape (n_samples, 2)\n",
    "            rmse = float(rmse_data[-1, 1])  # Get last RMSE value from column 1\n",
    "\n",
    "        if print_vars:\n",
    "            print_matlab_workspace(eng, title=\"MATLAB Workspace After Simulation\")\n",
    "\n",
    "        return rmse\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Simulation error with Kp={Kp}, Ki={Ki}, Kd={Kd}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"Simulink runner function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Q-Learning Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter ranges and discretization\n",
    "KP_MIN, KP_MAX, KP_STEP = 0.0, 5.0, 0.1\n",
    "KI_MIN, KI_MAX, KI_STEP = 0.0, 5.0, 0.1\n",
    "KD_MIN, KD_MAX, KD_STEP = 0.0, 5.0, 0.1\n",
    "\n",
    "# Create parameter ranges\n",
    "kp_range = np.arange(KP_MIN, KP_MAX + KP_STEP/2, KP_STEP)\n",
    "ki_range = np.arange(KI_MIN, KI_MAX + KI_STEP/2, KI_STEP)\n",
    "kd_range = np.arange(KD_MIN, KD_MAX + KD_STEP/2, KD_STEP)\n",
    "\n",
    "print(f\"Kp range: {len(kp_range)} values from {KP_MIN} to {KP_MAX}\")\n",
    "print(f\"Ki range: {len(ki_range)} values from {KI_MIN} to {KI_MAX}\")\n",
    "print(f\"Kd range: {len(kd_range)} values from {KD_MIN} to {KD_MAX}\")\n",
    "print(f\"Total state space size: {len(kp_range) * len(ki_range) * len(kd_range)}\")\n",
    "\n",
    "# Define action space\n",
    "# Actions: 0=increase Kp, 1=decrease Kp, 2=increase Ki, 3=decrease Ki, 4=increase Kd, 5=decrease Kd\n",
    "NUM_ACTIONS = 6\n",
    "\n",
    "# Q-Learning hyperparameters\n",
    "ALPHA = 0.1  # Learning rate\n",
    "GAMMA = 0.95  # Discount factor\n",
    "EPSILON = 1.0  # Initial exploration rate\n",
    "EPSILON_DECAY = 0.995  # Exploration decay\n",
    "EPSILON_MIN = 0.01  # Minimum exploration rate\n",
    "NUM_EPISODES = 2000  # Number of training episodes\n",
    "\n",
    "# Early stopping configuration\n",
    "TARGET_RMSE = 0.00001  # Target RMSE for early stopping\n",
    "ENABLE_EARLY_STOPPING = True  # Enable/disable early stopping\n",
    "\n",
    "# Initialize Q-table as dictionary\n",
    "Q_table = defaultdict(lambda: np.zeros(NUM_ACTIONS))\n",
    "\n",
    "print(f\"\\nQ-Learning Configuration:\")\n",
    "print(f\"  Learning rate (alpha): {ALPHA}\")\n",
    "print(f\"  Discount factor (gamma): {GAMMA}\")\n",
    "print(f\"  Initial exploration rate: {EPSILON}\")\n",
    "print(f\"  Epsilon decay: {EPSILON_DECAY}\")\n",
    "print(f\"  Training episodes: {NUM_EPISODES}\")\n",
    "print(f\"  Early stopping enabled: {ENABLE_EARLY_STOPPING}\")\n",
    "if ENABLE_EARLY_STOPPING:\n",
    "    print(f\"  Target RMSE: {TARGET_RMSE:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_to_string(action):\n",
    "    \"\"\"Convert action index to readable description.\"\"\"\n",
    "    action_map = {\n",
    "        0: \"â†‘ Increase Kp\",\n",
    "        1: \"â†“ Decrease Kp\",\n",
    "        2: \"â†‘ Increase Ki\",\n",
    "        3: \"â†“ Decrease Ki\",\n",
    "        4: \"â†‘ Increase Kd\",\n",
    "        5: \"â†“ Decrease Kd\"\n",
    "    }\n",
    "    return action_map.get(action, f\"Unknown action {action}\")\n",
    "\n",
    "def calculate_q_stats(Q_table):\n",
    "    \"\"\"Calculate Q-value statistics across entire table.\"\"\"\n",
    "    all_q_values = []\n",
    "    for state in Q_table:\n",
    "        all_q_values.extend(Q_table[state])\n",
    "\n",
    "    if not all_q_values:\n",
    "        return {'avg': 0, 'max': 0, 'min': 0, 'std': 0}\n",
    "\n",
    "    all_q_values = np.array(all_q_values)\n",
    "    return {\n",
    "        'avg': np.mean(all_q_values),\n",
    "        'max': np.max(all_q_values),\n",
    "        'min': np.min(all_q_values),\n",
    "        'std': np.std(all_q_values)\n",
    "    }\n",
    "\n",
    "def print_matlab_workspace(eng, title=\"MATLAB Workspace Variables\"):\n",
    "    \"\"\"\n",
    "    Print all variables in MATLAB workspace.\n",
    "\n",
    "    Args:\n",
    "        eng: MATLAB engine instance\n",
    "        title: Title to display before listing variables\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get all variable names in base workspace\n",
    "        var_names = eng.eval(\"who\", nargout=1)\n",
    "\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"{title}\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        if not var_names:\n",
    "            print(\"  (No variables in workspace)\")\n",
    "            return\n",
    "\n",
    "        # Print each variable with its type and size\n",
    "        for var_name in var_names:\n",
    "            try:\n",
    "                # Get variable info\n",
    "                var_value = eng.evalin('base', var_name)\n",
    "                var_class = eng.evalin('base', f\"class({var_name})\")\n",
    "                var_size = eng.evalin('base', f\"size({var_name})\")\n",
    "\n",
    "                # Format size as string\n",
    "                if hasattr(var_size, '__iter__'):\n",
    "                    size_str = 'x'.join(map(str, map(int, var_size)))\n",
    "                else:\n",
    "                    size_str = str(int(var_size))\n",
    "\n",
    "                print(f\"  {var_name:20s} | {var_class:15s} | size: {size_str}\")\n",
    "\n",
    "                # For small numeric scalars, show the value\n",
    "                if var_class == 'double' and size_str == '1x1':\n",
    "                    try:\n",
    "                        val = float(var_value)\n",
    "                        print(f\"    â””â”€ value: {val:.6f}\")\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  {var_name:20s} | (error retrieving info: {str(e)})\")\n",
    "\n",
    "        print(f\"{'='*70}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing MATLAB workspace: {str(e)}\")\n",
    "\n",
    "print(\"Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Helper Functions for Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging and Logging Configuration\n",
    "VERBOSE_LEVEL = 1  # 0=minimal, 1=per-episode+events (DEFAULT), 2=per-step, 3=full debug\n",
    "LOG_Q_VALUES = True  # Track Q-value evolution (user priority)\n",
    "TRACK_CONVERGENCE = True  # Monitor parameter convergence (user priority)\n",
    "CONVERGENCE_WINDOW = 50   # Window size for convergence detection\n",
    "CONVERGENCE_THRESHOLD = 0.001  # RMSE variance threshold for convergence\n",
    "\n",
    "# MATLAB Workspace Debugging\n",
    "PRINT_MATLAB_VARS = True  # Print MATLAB workspace variables after simulation\n",
    "PRINT_MATLAB_VARS_FREQUENCY = 'first'  # Options: 'always', 'first', 'verbose', 'never'\n",
    "\n",
    "print(\"Debugging configuration loaded:\")\n",
    "print(f\"  VERBOSE_LEVEL: {VERBOSE_LEVEL}\")\n",
    "print(f\"  LOG_Q_VALUES: {LOG_Q_VALUES}\")\n",
    "print(f\"  TRACK_CONVERGENCE: {TRACK_CONVERGENCE}\")\n",
    "print(f\"  PRINT_MATLAB_VARS: {PRINT_MATLAB_VARS} (frequency: {PRINT_MATLAB_VARS_FREQUENCY})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Debugging and Logging Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: State and Action Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_to_state(kp, ki, kd):\n",
    "    \"\"\"\n",
    "    Convert continuous PID parameters to state representation.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (kp_idx, ki_idx, kd_idx) - indices in the discretized ranges\n",
    "    \"\"\"\n",
    "    kp_idx = int(np.round((kp - KP_MIN) / KP_STEP))\n",
    "    ki_idx = int(np.round((ki - KI_MIN) / KI_STEP))\n",
    "    kd_idx = int(np.round((kd - KD_MIN) / KD_STEP))\n",
    "    \n",
    "    # Clamp to valid range\n",
    "    kp_idx = np.clip(kp_idx, 0, len(kp_range) - 1)\n",
    "    ki_idx = np.clip(ki_idx, 0, len(ki_range) - 1)\n",
    "    kd_idx = np.clip(kd_idx, 0, len(kd_range) - 1)\n",
    "    \n",
    "    return (kp_idx, ki_idx, kd_idx)\n",
    "\n",
    "def state_to_params(state):\n",
    "    \"\"\"\n",
    "    Convert state representation to continuous PID parameters.\n",
    "    \n",
    "    Args:\n",
    "        state: tuple (kp_idx, ki_idx, kd_idx)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (Kp, Ki, Kd) continuous values\n",
    "    \"\"\"\n",
    "    kp_idx, ki_idx, kd_idx = state\n",
    "    kp = kp_range[kp_idx]\n",
    "    ki = ki_range[ki_idx]\n",
    "    kd = kd_range[kd_idx]\n",
    "    return (kp, ki, kd)\n",
    "\n",
    "def take_action(state, action):\n",
    "    \"\"\"\n",
    "    Take an action in the state space.\n",
    "    \n",
    "    Args:\n",
    "        state: tuple (kp_idx, ki_idx, kd_idx)\n",
    "        action: int from 0 to 5\n",
    "        \n",
    "    Returns:\n",
    "        tuple: new_state\n",
    "    \"\"\"\n",
    "    kp_idx, ki_idx, kd_idx = state\n",
    "    \n",
    "    if action == 0:  # Increase Kp\n",
    "        kp_idx = min(kp_idx + 1, len(kp_range) - 1)\n",
    "    elif action == 1:  # Decrease Kp\n",
    "        kp_idx = max(kp_idx - 1, 0)\n",
    "    elif action == 2:  # Increase Ki\n",
    "        ki_idx = min(ki_idx + 1, len(ki_range) - 1)\n",
    "    elif action == 3:  # Decrease Ki\n",
    "        ki_idx = max(ki_idx - 1, 0)\n",
    "    elif action == 4:  # Increase Kd\n",
    "        kd_idx = min(kd_idx + 1, len(kd_range) - 1)\n",
    "    elif action == 5:  # Decrease Kd\n",
    "        kd_idx = max(kd_idx - 1, 0)\n",
    "    \n",
    "    return (kp_idx, ki_idx, kd_idx)\n",
    "\n",
    "print(\"State and action functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward(rmse):\n",
    "    \"\"\"\n",
    "    Convert RMSE to reward signal.\n",
    "    Lower RMSE = higher reward (less error = more positive reward).\n",
    "    \n",
    "    Args:\n",
    "        rmse (float): Root Mean Square Error from simulation\n",
    "        \n",
    "    Returns:\n",
    "        float: Reward value\n",
    "    \"\"\"\n",
    "    if rmse is None:\n",
    "        return -100  # Penalty for simulation failure\n",
    "    return -rmse  # Negative RMSE as reward (minimizing RMSE)\n",
    "\n",
    "print(\"Reward function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Q-Learning Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training variables\n",
    "epsilon = EPSILON\n",
    "target_achieved = False  # Flag for early stopping\n",
    "training_history = {\n",
    "    'episode': [],\n",
    "    'rmse': [],\n",
    "    'best_rmse': [],\n",
    "    'kp': [],\n",
    "    'ki': [],\n",
    "    'kd': [],\n",
    "    'epsilon': []\n",
    "}\n",
    "\n",
    "# Q-Learning tracking (user priority: Q-learning internals)\n",
    "q_value_history = {\n",
    "    'episode': [],\n",
    "    'avg_q_value': [],\n",
    "    'max_q_value': [],\n",
    "    'min_q_value': [],\n",
    "    'q_value_std': []\n",
    "}\n",
    "\n",
    "# Convergence tracking (user priority: parameter convergence)\n",
    "convergence_status = {\n",
    "    'is_converged': False,\n",
    "    'convergence_episode': None,\n",
    "    'steps_without_improvement': 0\n",
    "}\n",
    "\n",
    "simulation_stats = {\n",
    "    'total_sims': 0,\n",
    "    'successful_sims': 0,\n",
    "    'failed_sims': 0\n",
    "}\n",
    "\n",
    "best_rmse = float('inf')\n",
    "best_params = None\n",
    "\n",
    "# Initial state: use middle values of parameter ranges\n",
    "initial_kp_idx = 0\n",
    "initial_ki_idx = 0\n",
    "initial_kd_idx = 0\n",
    "initial_state = (initial_kp_idx, initial_ki_idx, initial_kd_idx)\n",
    "\n",
    "print(f\"Starting Q-Learning Training...\")\n",
    "print(f\"Initial state parameters: {state_to_params(initial_state)}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "    state = initial_state\n",
    "    episode_rmse = []\n",
    "    episode_td_errors = []\n",
    "    episode_q_updates = []\n",
    "    \n",
    "    # Episode loop\n",
    "    for step in range(10):  # 10 steps per episode\n",
    "        # Epsilon-greedy action selection\n",
    "        is_explore = np.random.random() < epsilon\n",
    "        if is_explore:\n",
    "            # Explore: random action\n",
    "            action = np.random.randint(0, NUM_ACTIONS)\n",
    "        else:\n",
    "            # Exploit: best known action\n",
    "            action = np.argmax(Q_table[state])\n",
    "        \n",
    "        # Take action and get new state\n",
    "        new_state = take_action(state, action)\n",
    "        \n",
    "        # Determine if we should print MATLAB variables for this simulation\n",
    "        should_print_vars = False\n",
    "        if PRINT_MATLAB_VARS:\n",
    "            if PRINT_MATLAB_VARS_FREQUENCY == 'always':\n",
    "                should_print_vars = True\n",
    "            elif PRINT_MATLAB_VARS_FREQUENCY == 'first' and simulation_stats['total_sims'] == 0:\n",
    "                should_print_vars = True\n",
    "            elif PRINT_MATLAB_VARS_FREQUENCY == 'verbose' and VERBOSE_LEVEL >= 2:\n",
    "                should_print_vars = True\n",
    "        \n",
    "        # Run Simulink and get reward\n",
    "        kp, ki, kd = state_to_params(new_state)\n",
    "        rmse = run_simulink(kp, ki, kd, eng, print_vars=should_print_vars)\n",
    "        reward = calculate_reward(rmse)\n",
    "        \n",
    "        # Track simulation statistics\n",
    "        simulation_stats['total_sims'] += 1\n",
    "        if rmse is not None:\n",
    "            simulation_stats['successful_sims'] += 1\n",
    "        else:\n",
    "            simulation_stats['failed_sims'] += 1\n",
    "            if VERBOSE_LEVEL >= 1:\n",
    "                print(f\"  âš  Simulation failed: Kp={kp:.4f}, Ki={ki:.4f}, Kd={kd:.4f}\")\n",
    "        \n",
    "        if rmse is not None:\n",
    "            episode_rmse.append(rmse)\n",
    "            \n",
    "            # Enhanced logging when best parameters update (KEY EVENT)\n",
    "            if rmse < best_rmse:\n",
    "                old_best = best_rmse\n",
    "                best_rmse = rmse\n",
    "                best_params = (kp, ki, kd)\n",
    "                convergence_status['steps_without_improvement'] = 0\n",
    "\n",
    "                if VERBOSE_LEVEL >= 1:\n",
    "                    improvement = ((old_best - best_rmse) / old_best) * 100\n",
    "                    print(f\"  âœ“ NEW BEST! RMSE: {old_best:.6f} â†’ {best_rmse:.6f} ({improvement:.2f}% better)\")\n",
    "                    print(f\"    Parameters: Kp={kp:.4f}, Ki={ki:.4f}, Kd={kd:.4f}\")\n",
    "                \n",
    "                # Check if target RMSE achieved for early stopping\n",
    "                if ENABLE_EARLY_STOPPING and best_rmse <= TARGET_RMSE:\n",
    "                    print(f\"\\n{'='*70}\")\n",
    "                    print(f\"ðŸŽ¯ TARGET ACHIEVED!\")\n",
    "                    print(f\"  Target RMSE: {TARGET_RMSE:.6f}\")\n",
    "                    print(f\"  Achieved RMSE: {best_rmse:.6f}\")\n",
    "                    print(f\"  Episode: {episode + 1}/{NUM_EPISODES}\")\n",
    "                    print(f\"  Parameters: Kp={kp:.6f}, Ki={ki:.6f}, Kd={kd:.6f}\")\n",
    "                    print(f\"  Training time: {time.time() - start_time:.1f}s\")\n",
    "                    print(f\"{'='*70}\")\n",
    "                    target_achieved = True\n",
    "                    break  # Exit step loop\n",
    "            else:\n",
    "                convergence_status['steps_without_improvement'] += 1\n",
    "        \n",
    "        # Store Q-value before update for tracking (user priority)\n",
    "        if LOG_Q_VALUES:\n",
    "            q_before = Q_table[state][action]\n",
    "        \n",
    "        # Q-Learning update\n",
    "        max_next_q = np.max(Q_table[new_state])\n",
    "        Q_table[state][action] = Q_table[state][action] + ALPHA * (reward + GAMMA * max_next_q - Q_table[state][action])\n",
    "        \n",
    "        # Track Q-value change (user priority: Q-learning internals)\n",
    "        if LOG_Q_VALUES:\n",
    "            q_after = Q_table[state][action]\n",
    "            td_error = reward + GAMMA * max_next_q - q_before\n",
    "            episode_td_errors.append(abs(td_error))\n",
    "            episode_q_updates.append(abs(q_after - q_before))\n",
    "        \n",
    "        state = new_state\n",
    "    \n",
    "    # Exit outer loop if target was achieved\n",
    "    if target_achieved:\n",
    "        break\n",
    "    \n",
    "    # Decay exploration rate\n",
    "    epsilon = max(EPSILON_MIN, epsilon * EPSILON_DECAY)\n",
    "    \n",
    "    # Calculate Q-value statistics for this episode (user priority)\n",
    "    if LOG_Q_VALUES:\n",
    "        all_q_values = []\n",
    "        for s in Q_table:\n",
    "            all_q_values.extend(Q_table[s])\n",
    "\n",
    "        q_value_history['episode'].append(episode + 1)\n",
    "        q_value_history['avg_q_value'].append(np.mean(all_q_values))\n",
    "        q_value_history['max_q_value'].append(np.max(all_q_values))\n",
    "        q_value_history['min_q_value'].append(np.min(all_q_values))\n",
    "        q_value_history['q_value_std'].append(np.std(all_q_values))\n",
    "\n",
    "        avg_td_error = np.mean(episode_td_errors) if episode_td_errors else 0\n",
    "        avg_q_update = np.mean(episode_q_updates) if episode_q_updates else 0\n",
    "    \n",
    "    # Record episode statistics\n",
    "    avg_rmse = np.mean(episode_rmse) if episode_rmse else None\n",
    "    if avg_rmse is not None:\n",
    "        training_history['episode'].append(episode + 1)\n",
    "        training_history['rmse'].append(avg_rmse)\n",
    "        training_history['best_rmse'].append(best_rmse)\n",
    "        training_history['epsilon'].append(epsilon)\n",
    "        \n",
    "        if best_params:\n",
    "            training_history['kp'].append(best_params[0])\n",
    "            training_history['ki'].append(best_params[1])\n",
    "            training_history['kd'].append(best_params[2])\n",
    "    \n",
    "    # Enhanced per-episode logging (VERBOSE_LEVEL >= 1, default)\n",
    "    if VERBOSE_LEVEL >= 1 and avg_rmse is not None:\n",
    "        success_rate = (simulation_stats['successful_sims'] /\n",
    "                       simulation_stats['total_sims'] * 100)\n",
    "\n",
    "        print(f\"Ep {episode + 1:3d}/{NUM_EPISODES} | \"\n",
    "              f\"RMSE: {avg_rmse:.6f} | \"\n",
    "              f\"Best: {best_rmse:.6f} | \"\n",
    "              f\"Îµ: {epsilon:.4f} | \"\n",
    "              f\"Success: {success_rate:.1f}%\", end=\"\")\n",
    "\n",
    "        if LOG_Q_VALUES:\n",
    "            print(f\" | Q-avg: {q_value_history['avg_q_value'][-1]:.4f} | \"\n",
    "                  f\"TD-err: {avg_td_error:.4f}\")\n",
    "        else:\n",
    "            print()\n",
    "    \n",
    "    # Check convergence (user priority: parameter convergence)\n",
    "    if TRACK_CONVERGENCE and len(training_history['rmse']) >= CONVERGENCE_WINDOW:\n",
    "        recent_rmse = training_history['rmse'][-CONVERGENCE_WINDOW:]\n",
    "        rmse_variance = np.var(recent_rmse)\n",
    "\n",
    "        if rmse_variance < CONVERGENCE_THRESHOLD and not convergence_status['is_converged']:\n",
    "            convergence_status['is_converged'] = True\n",
    "            convergence_status['convergence_episode'] = episode + 1\n",
    "\n",
    "            if VERBOSE_LEVEL >= 1:\n",
    "                print(f\"  ðŸŽ¯ CONVERGENCE DETECTED at Episode {episode + 1}\")\n",
    "                print(f\"    RMSE variance over last {CONVERGENCE_WINDOW} episodes: {rmse_variance:.6f}\")\n",
    "                print(f\"    Steps without improvement: {convergence_status['steps_without_improvement']}\")\n",
    "\n",
    "elapsed_total = time.time() - start_time\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTraining completed in {elapsed_total:.1f} seconds\")\n",
    "print(f\"Episodes run: {episode + 1}/{NUM_EPISODES}\", end=\"\")\n",
    "if target_achieved:\n",
    "    episodes_saved = NUM_EPISODES - (episode + 1)\n",
    "    print(f\" (early stopping - saved {episodes_saved} episodes)\")\n",
    "else:\n",
    "    print()\n",
    "print(f\"\\nBest Parameters Found:\")\n",
    "print(f\"  Kp = {best_params[0]:.4f}\")\n",
    "print(f\"  Ki = {best_params[1]:.4f}\")\n",
    "print(f\"  Kd = {best_params[2]:.4f}\")\n",
    "print(f\"  RMSE = {best_rmse:.6f}\")\n",
    "if target_achieved:\n",
    "    print(f\"\\nâœ“ Target RMSE of {TARGET_RMSE:.6f} was achieved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Q-value evolution (user priority)\n",
    "if LOG_Q_VALUES and len(q_value_history['episode']) > 0:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Plot 1: Q-value evolution over episodes\n",
    "    ax1.plot(q_value_history['episode'], q_value_history['avg_q_value'],\n",
    "             label='Avg Q-value', linewidth=2, color='blue')\n",
    "    ax1.fill_between(q_value_history['episode'],\n",
    "                     q_value_history['min_q_value'],\n",
    "                     q_value_history['max_q_value'],\n",
    "                     alpha=0.2, color='blue', label='Min-Max Range')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Q-Value')\n",
    "    ax1.set_title('Q-Value Evolution During Training')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: RMSE vs Q-value correlation\n",
    "    ax2.scatter(q_value_history['avg_q_value'][:len(training_history['best_rmse'])], \n",
    "                training_history['best_rmse'][:len(q_value_history['avg_q_value'])],\n",
    "                alpha=0.6, s=30, color='red')\n",
    "    ax2.set_xlabel('Average Q-Value')\n",
    "    ax2.set_ylabel('Best RMSE')\n",
    "    ax2.set_title('Q-Value vs Performance Correlation')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('q_value_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nQ-value analysis saved as 'q_value_analysis.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7b: Q-Value Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print focused training statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING SUMMARY - CONVERGENCE & Q-LEARNING ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Convergence Analysis\n",
    "print(f\"\\nConvergence Status:\")\n",
    "if convergence_status['is_converged']:\n",
    "    print(f\"  âœ“ Converged at episode {convergence_status['convergence_episode']}\")\n",
    "    print(f\"  Remaining episodes after convergence: \"\n",
    "          f\"{NUM_EPISODES - convergence_status['convergence_episode']}\")\n",
    "else:\n",
    "    print(f\"  âš  Did not converge within {NUM_EPISODES} episodes\")\n",
    "print(f\"  Final steps without improvement: {convergence_status['steps_without_improvement']}\")\n",
    "\n",
    "# Q-Learning Internals\n",
    "if LOG_Q_VALUES and len(q_value_history['episode']) > 0:\n",
    "    print(f\"\\nQ-Value Evolution:\")\n",
    "    print(f\"  Initial avg Q-value: {q_value_history['avg_q_value'][0]:.6f}\")\n",
    "    print(f\"  Final avg Q-value: {q_value_history['avg_q_value'][-1]:.6f}\")\n",
    "    print(f\"  Q-value range: [{q_value_history['min_q_value'][-1]:.6f}, \"\n",
    "          f\"{q_value_history['max_q_value'][-1]:.6f}]\")\n",
    "    print(f\"  Final Q-value std dev: {q_value_history['q_value_std'][-1]:.6f}\")\n",
    "\n",
    "    # Detect if Q-values are converging\n",
    "    if len(q_value_history['avg_q_value']) >= 20:\n",
    "        recent_change = abs(q_value_history['avg_q_value'][-1] -\n",
    "                          q_value_history['avg_q_value'][-20])\n",
    "        print(f\"  Q-value change (last 20 episodes): {recent_change:.6f}\")\n",
    "\n",
    "print(f\"\\nState Space Exploration:\")\n",
    "print(f\"  Unique states visited: {len(Q_table)}\")\n",
    "total_states = len(kp_range) * len(ki_range) * len(kd_range)\n",
    "print(f\"  State space coverage: {len(Q_table)/total_states*100:.2f}% \"\n",
    "      f\"({len(Q_table)}/{total_states})\")\n",
    "\n",
    "print(f\"\\nSimulation Statistics:\")\n",
    "success_rate = (simulation_stats['successful_sims']/simulation_stats['total_sims']*100\n",
    "                if simulation_stats['total_sims'] > 0 else 0)\n",
    "print(f\"  Total simulations: {simulation_stats['total_sims']}\")\n",
    "print(f\"  Success rate: {success_rate:.2f}% \"\n",
    "      f\"({simulation_stats['successful_sims']}/{simulation_stats['total_sims']})\")\n",
    "if simulation_stats['failed_sims'] > 0:\n",
    "    print(f\"  âš  Failed simulations: {simulation_stats['failed_sims']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7a: Post-Training Debug Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: RMSE over episodes\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(training_history['episode'], training_history['rmse'], label='Episode Avg RMSE', alpha=0.7)\n",
    "ax1.plot(training_history['episode'], training_history['best_rmse'], label='Best RMSE', linewidth=2)\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('RMSE')\n",
    "ax1.set_title('Training Progress: RMSE over Episodes')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Parameter evolution - Kp\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(training_history['episode'], training_history['kp'], color='red', label='Kp', linewidth=2)\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Kp Value')\n",
    "ax2.set_title('Proportional Gain (Kp) Evolution')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=best_params[0], color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Plot 3: Parameter evolution - Ki\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(training_history['episode'], training_history['ki'], color='green', label='Ki', linewidth=2)\n",
    "ax3.set_xlabel('Episode')\n",
    "ax3.set_ylabel('Ki Value')\n",
    "ax3.set_title('Integral Gain (Ki) Evolution')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.axhline(y=best_params[1], color='g', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Plot 4: Parameter evolution - Kd\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(training_history['episode'], training_history['kd'], color='blue', label='Kd', linewidth=2)\n",
    "ax4.set_xlabel('Episode')\n",
    "ax4.set_ylabel('Kd Value')\n",
    "ax4.set_title('Derivative Gain (Kd) Evolution')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.axhline(y=best_params[2], color='b', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('pid_training_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTraining visualization saved as 'pid_training_results.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Q-Table Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Q-table\n",
    "q_values = []\n",
    "for state in Q_table:\n",
    "    q_values.extend(Q_table[state])\n",
    "\n",
    "q_values = np.array(q_values)\n",
    "\n",
    "print(\"Q-Table Statistics:\")\n",
    "print(f\"  Total states explored: {len(Q_table)}\")\n",
    "print(f\"  Total Q-values: {len(q_values)}\")\n",
    "print(f\"  Mean Q-value: {np.mean(q_values):.6f}\")\n",
    "print(f\"  Min Q-value: {np.min(q_values):.6f}\")\n",
    "print(f\"  Max Q-value: {np.max(q_values):.6f}\")\n",
    "print(f\"  Std Q-value: {np.std(q_values):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Validation Run with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nValidation Run with Best Parameters Found\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Run simulation with best parameters (print MATLAB variables during validation)\n",
    "final_rmse = run_simulink(best_params[0], best_params[1], best_params[2], eng, print_vars=True)\n",
    "\n",
    "print(f\"\\nFinal Validation Results:\")\n",
    "print(f\"  Kp = {best_params[0]:.6f}\")\n",
    "print(f\"  Ki = {best_params[1]:.6f}\")\n",
    "print(f\"  Kd = {best_params[2]:.6f}\")\n",
    "print(f\"  RMSE = {final_rmse:.6f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compare with initial parameters\n",
    "initial_kp, initial_ki, initial_kd = state_to_params(initial_state)\n",
    "initial_rmse = run_simulink(initial_kp, initial_ki, initial_kd, eng)\n",
    "\n",
    "print(f\"\\nComparison with Initial Parameters:\")\n",
    "print(f\"  Initial: Kp={initial_kp:.6f}, Ki={initial_ki:.6f}, Kd={initial_kd:.6f}, RMSE={initial_rmse:.6f}\")\n",
    "print(f\"  Optimized: Kp={best_params[0]:.6f}, Ki={best_params[1]:.6f}, Kd={best_params[2]:.6f}, RMSE={final_rmse:.6f}\")\n",
    "improvement = ((initial_rmse - final_rmse) / initial_rmse) * 100\n",
    "print(f\"  Improvement: {improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to file\n",
    "import json\n",
    "\n",
    "results = {\n",
    "    'best_parameters': {\n",
    "        'Kp': float(best_params[0]),\n",
    "        'Ki': float(best_params[1]),\n",
    "        'Kd': float(best_params[2]),\n",
    "    },\n",
    "    'best_rmse': float(best_rmse),\n",
    "    'training_episodes': NUM_EPISODES,\n",
    "    'training_time_seconds': elapsed_total,\n",
    "    'improvement_percent': improvement\n",
    "}\n",
    "\n",
    "with open('pid_optimization_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "with open('RL_gains.txt', 'w') as f:\n",
    "    f.write(f\"{best_params[0]:.6f},\\n\")\n",
    "    f.write(f\"{best_params[1]:.6f},\\n\")\n",
    "    f.write(f\"{best_params[2]:.6f}\\n\")\n",
    "\n",
    "print(\"\\nResults exported to 'pid_optimization_results.json'\")\n",
    "print(\"\\nFinal Results Summary:\")\n",
    "print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close MATLAB engine\n",
    "print(\"Closing MATLAB engine...\")\n",
    "eng.quit()\n",
    "print(\"MATLAB engine closed successfully!\")\n",
    "print(\"\\nOptimization workflow completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
